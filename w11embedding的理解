1.对embedding的理解
首先，每个汉字占用一个维度的一个位置，那么多少汉字就有多少维度，再和权重相乘，数据量太大。
其次，embedding就是降维操作。比如缩小到300维度以内。
再次，降维到低维度空间中，如何映射。word2vec提供了一种方法：初始化权重矩阵，再通过简单查表操作。找到对应坐标。然后，再训练权重矩阵，如此循环。找到最佳映射的坐标。
最后，计算低维度中坐标的余弦相似度，夹角越小，汉字之间的关系越密切。
2.代码的分析
输入整段有效文章，提取所有汉字。按照频率进行排序。取频率最高的5000个汉字。
利用RNN进行训练。输入是依次截取文章的5到10个字。任意取其中一个字，那么该字的前后字，就是其最相关的字。整段文章循环结束，则训练完毕。可以重复多次，增加准确率。
输出每个字相关的汉字。
3.对上述图片的结果分析和认识
图片显示的就是降成2维之后，各个汉字在坐标中的位置。
汉字的距离显示了各自意义的相关性。
